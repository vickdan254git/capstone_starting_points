{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(r'..\\capstones\\ai-tech-capstone\\cohort-3\\data\\train.csv')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Week 6 - Build Candidate Models\n",
    "\n",
    "Over the previous two weeks, you built a good baseline model for prediction. Now you have a point of comparison for more sophisticated approaches which we expect to provide better performance, though often at the cost of higher computational complexity. This week, each member of the capstone team is going to build their own classifier with a di↵erent algorithm.\n",
    "\n",
    "Use the previous weeks’ instruction to guide you in the process."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some options to get you started.\n",
    "• Naive Bayes\n",
    "• K-Nearest Neighbor\n",
    "• Decision Tree\n",
    "• Random Forest\n",
    "• Gradient Boosting\n",
    "• XGBoost"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point you have a candidate model for predicting whether someone will subscribe to the term deposit. Now you need to use the data to tune hyper parameters with that model. Follow the same process that you used to tune the L2 Logistic Regression with grid search and k-fold cross-validation; however, your models might have multiple parameters that you should consider tuning. Be careful with adding too many parameters to the grid search because it will search every combination of parameter value that you provide. If you decide to search over multiple parameters and many values for each parameter, consider using a random search.\n",
    "\n",
    "Note: Hyper parameters are not constrained to arguments that you explicitly pass to your model. They also include data pre-processing, resampling (e.g. SMOTE), and any other transformation you make on the data. We recommend that you treate resampling as a hyperparameter. To simplify this process, produce a tuned candidate model with and without resampling to be evaluated in model selection next week."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
